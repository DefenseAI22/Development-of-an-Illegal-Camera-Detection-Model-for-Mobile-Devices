{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "606c847e-c473-47ac-91cd-4afd5cfb0aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#watch -n 1 nvidia-smi\n",
    "# 터미널에 실행해서 gpu 보면서 실행시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c320ad00-a52f-4552-8012-3e6ad02a66ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, distutils.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c003bfc-59bd-4a57-973d-4050045e1369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install pyyaml==5.1\n",
    "# # Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).\n",
    "# # See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
    "# !git clone 'https://github.com/facebookresearch/detectron2' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a60c2bab-7126-4d37-acbb-a8e7b2a32a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/conda/lib/python311.zip', '/opt/conda/lib/python3.11', '/opt/conda/lib/python3.11/lib-dynload', '', '/opt/conda/lib/python3.11/site-packages', '/home/jovyan/snukdt/AdelaiDet', '/opt/conda/lib/python3.11/site-packages/pydot-3.0.3-py3.11.egg', '/opt/conda/lib/python3.11/site-packages/future-1.0.0-py3.11.egg', '/opt/conda/lib/python3.11/site-packages/setuptools-75.6.0-py3.11.egg', '/home/jovyan/snukdt/BCNet', '/opt/conda/lib/python3.11/site-packages/Pillow-6.2.2-py3.11-linux-x86_64.egg', '/home/jovyan/snukdt', '/home/jovyan/snukdt/detectron2', '/opt/conda/lib/python3.11/site-packages/setuptools/_vendor']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5291686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist = distutils.core.run_setup(\"detectron2/setup.py\")\n",
    "# !python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
    "sys.path.insert(0, '/Users/parkjunhui/Desktop/Spresto/detectron2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8babfd92-9420-4eae-b2a9-399855e41beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import math\n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import logging\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import LazyConfig, instantiate, get_cfg\n",
    "from detectron2.data import (\n",
    "    MetadataCatalog,\n",
    "    DatasetCatalog,\n",
    "    build_detection_train_loader,\n",
    "    build_detection_test_loader\n",
    ")\n",
    "from detectron2.engine import SimpleTrainer, DefaultPredictor, hooks, DefaultTrainer, HookBase\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset, DatasetEvaluators\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.utils.events import CommonMetricPrinter, JSONWriter, TensorboardXWriter, EventStorage\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.structures import BoxMode\n",
    "import detectron2.data\n",
    "from detectron2.data import detection_utils as utils\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 로깅 설정\n",
    "setup_logger()\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.WARNING)\n",
    "logging.getLogger(\"PIL\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50368a76-03af-4c0b-85d5-52ae2d30de2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvcc: command not found\n",
      "torch:  2.4 ; cuda:  cu118\n",
      "detectron2: 0.6\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11366c55-7bea-432c-bc89-1b83d4cbfc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GPU device\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d1de718-8c2a-4247-8c6e-6db9d7131075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"Seed 설정 함수\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def apply_gaussian_noise(image, mean=0, std=25):\n",
    "    \"\"\"\n",
    "    이미지에 가우시안 노이즈를 추가하는 함수.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): 원본 이미지 (H x W x C)\n",
    "        mean (float): 노이즈의 평균값\n",
    "        std (float): 노이즈의 표준 편차\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 노이즈가 추가된 이미지\n",
    "    \"\"\"\n",
    "    # 정규 분포에서 노이즈 생성\n",
    "    noise = np.random.normal(mean, std, image.shape).astype(np.float32)\n",
    "    # 원본 이미지에 노이즈 추가\n",
    "    noisy_image = image.astype(np.float32) + noise\n",
    "    # 픽셀 값 범위를 0~255로 클리핑\n",
    "    return np.clip(noisy_image, 0, 255).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "20a3bcf8-0360-4c6c-9f97-2858dccc44e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import cv2\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.data import detection_utils as utils\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2.data import build_detection_train_loader, build_detection_test_loader\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "from detectron2 import model_zoo\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# 1) get_custom_dicts\n",
    "# ---------------------------------------------------------------------------- #\n",
    "def get_custom_dicts(img_dir, json_file):\n",
    "    \"\"\"\n",
    "    Detectron2 형식의 데이터셋을 생성. 클래스가 1개로 고정된 데이터셋 처리.\n",
    "\n",
    "    Args:\n",
    "        img_dir (str): 이미지 파일이 위치한 디렉토리 경로\n",
    "        json_file (str): COCO 형식 JSON 파일 경로\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: Detectron2 형식의 데이터셋\n",
    "    \"\"\"\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 이미지 및 어노테이션 매핑\n",
    "    images = {img[\"id\"]: img for img in data[\"images\"]}\n",
    "    annotations = {}\n",
    "\n",
    "    # 어노테이션 필터링 및 처리\n",
    "    for ann in data[\"annotations\"]:\n",
    "        image_id = ann[\"image_id\"]\n",
    "        ann_copy = ann.copy()\n",
    "\n",
    "        # 단일 클래스이므로 모든 category_id를 0으로 설정\n",
    "        ann_copy[\"category_id\"] = 0\n",
    "        annotations.setdefault(image_id, []).append(ann_copy)\n",
    "\n",
    "    # Detectron2 형식 데이터셋 생성\n",
    "    dataset_dicts = []\n",
    "    for image_id, img_info in images.items():\n",
    "        filtered_annotations = [\n",
    "            {\n",
    "                \"bbox\": ann[\"bbox\"],\n",
    "                \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                \"category_id\": ann[\"category_id\"]\n",
    "            }\n",
    "            for ann in annotations.get(image_id, [])\n",
    "        ]\n",
    "\n",
    "        # 어노테이션이 없는 이미지는 제외\n",
    "        if filtered_annotations:\n",
    "            record = {\n",
    "                \"file_name\": os.path.join(img_dir, img_info[\"file_name\"]),\n",
    "                \"image_id\": image_id,\n",
    "                \"height\": img_info[\"height\"],\n",
    "                \"width\": img_info[\"width\"],\n",
    "                \"annotations\": filtered_annotations,\n",
    "            }\n",
    "            dataset_dicts.append(record)\n",
    "\n",
    "    return dataset_dicts\n",
    "    \n",
    "# ---------------------------------------------------------------------------- #\n",
    "# 2) 데이터셋 Split\n",
    "# ---------------------------------------------------------------------------- #\n",
    "def split_dataset(dataset, train_size=2000, val_size=318, test_size=319, seed=42):\n",
    "    \"\"\"\n",
    "    데이터를 훈련, 검증, 테스트 세트로 나눕니다.\n",
    "    \n",
    "    Args:\n",
    "        dataset (list): 전체 데이터셋\n",
    "        train_size (int): 훈련 데이터 개수\n",
    "        val_size (int): 검증 데이터 개수\n",
    "        test_size (int): 테스트 데이터 개수\n",
    "        seed (int): 무작위성을 고정하기 위한 시드\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train, val, test) 데이터셋\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    random.shuffle(dataset)\n",
    "    return (\n",
    "        dataset[:train_size], \n",
    "        dataset[train_size:train_size + val_size], \n",
    "        dataset[train_size + val_size:]\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# 3) 커스텀 Mapper (320x320 Resize)\n",
    "# ---------------------------------------------------------------------------- #\n",
    "from detectron2.data.transforms import AugInput\n",
    "\n",
    "def custom_mapper(dataset_dict):\n",
    "    \"\"\"\n",
    "    Detectron2 데이터셋 매퍼 함수, 이미지와 어노테이션을 증강 처리.\n",
    "\n",
    "    Args:\n",
    "        dataset_dict (dict): 데이터셋 정보 딕셔너리\n",
    "\n",
    "    Returns:\n",
    "        dict: 변환된 데이터셋\n",
    "    \"\"\"\n",
    "    dataset_dict = dataset_dict.copy()\n",
    "    image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n",
    "\n",
    "    # Augmentation 설정\n",
    "    augmentation = T.AugmentationList([\n",
    "        T.Resize((320, 320)),               # Resize\n",
    "        T.RandomFlip(prob=0.5),            # 랜덤 플립\n",
    "        T.RandomBrightness(0.8, 1.2),      # 밝기 조정\n",
    "        T.RandomContrast(0.8, 1.2),        # 대비 조정\n",
    "        T.RandomSaturation(0.8, 1.2),      # 채도 조정\n",
    "    ])\n",
    "\n",
    "    # AugInput 객체 생성\n",
    "    aug_input = AugInput(image)\n",
    "    transform = augmentation(aug_input)  # Transform 적용\n",
    "    image_augmented = aug_input.image    # 변환된 이미지 가져오기\n",
    "\n",
    "    # 가우시안 노이즈 추가 (50% 확률로 적용)\n",
    "    if np.random.rand() < 0.5:\n",
    "        image_augmented = apply_gaussian_noise(image_augmented, mean=0, std=25)\n",
    "\n",
    "    # 바운딩 박스 변환\n",
    "    if \"annotations\" in dataset_dict:\n",
    "        for anno in dataset_dict[\"annotations\"]:\n",
    "            x1, y1, w, h = anno[\"bbox\"]\n",
    "            x2, y2 = x1 + w, y1 + h\n",
    "            bbox = np.array([[x1, y1, x2, y2]], dtype=\"float32\")\n",
    "            transformed_bbox = transform.apply_box(bbox)[0]\n",
    "            anno[\"bbox\"] = [\n",
    "                transformed_bbox[0],\n",
    "                transformed_bbox[1],\n",
    "                transformed_bbox[2] - transformed_bbox[0],\n",
    "                transformed_bbox[3] - transformed_bbox[1],\n",
    "            ]\n",
    "\n",
    "    # 이미지를 텐서로 변환\n",
    "    dataset_dict[\"image\"] = torch.as_tensor(image_augmented.transpose(2, 0, 1).astype(\"float32\"))\n",
    "    dataset_dict[\"height\"], dataset_dict[\"width\"] = 320, 320\n",
    "\n",
    "    # Detectron2 인스턴스 생성\n",
    "    if \"annotations\" in dataset_dict:\n",
    "        annos = [\n",
    "            utils.transform_instance_annotations(\n",
    "                obj, transforms=[], image_size=(320, 320)\n",
    "            )\n",
    "            for obj in dataset_dict[\"annotations\"]\n",
    "        ]\n",
    "        dataset_dict[\"instances\"] = utils.annotations_to_instances(\n",
    "            annos, (320, 320)\n",
    "        )\n",
    "\n",
    "    return dataset_dict\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# 4) DataLoader 빌더\n",
    "# ---------------------------------------------------------------------------- #\n",
    "def build_train_loader(cfg):\n",
    "    return build_detection_train_loader(cfg, mapper=custom_mapper)\n",
    "\n",
    "# def build_test_loader(cfg):\n",
    "#     return build_detection_test_loader(cfg, dataset_name=cfg.DATASETS.TEST[0], mapper=custom_mapper)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# 5) CustomTrainer\n",
    "# ---------------------------------------------------------------------------- #\n",
    "class CustomTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        return build_train_loader(cfg)\n",
    "\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name):\n",
    "        return COCOEvaluator(dataset_name, cfg, True, output_dir=cfg.OUTPUT_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72a4d285-6d87-4393-a0c1-3e9c59d345b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/snukdt\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "# 삭제할 인덱스 정의\n",
    "delete_indices = set()\n",
    "\n",
    "# 크기 0인 바운딩박스 포함하는 이미지 삭제\n",
    "delete_indices.update([92, 152, 261, 383, 444, 447, 576, 715, 898, 974, 1049, 1050, 1149, 1354, 1355, 1378])\n",
    "# width 40 넘는 것 삭제\n",
    "delete_indices.update([233,  276, 1062, 1112, 1140])\n",
    "\n",
    "print(len(delete_indices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afe3983-89d9-45e7-a4aa-29e47a81c7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train=1000, Val=184, Test=184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "namespace(name='cropped_test', thing_classes=['object'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "# 6) 데이터셋 등록 및 Split\n",
    "# ---------------------------------------------------------------------------- #\n",
    "img_dir = \"./cropped_output/image\"\n",
    "json_file = \"./cropped_output/json/cropped_annotations_xywh.json\"\n",
    "\n",
    "\n",
    "# 클래스 이름 및 색상 설정 (Hiddenlens only)\n",
    "class_names = [\"Hiddenlens\"]\n",
    "\n",
    "\n",
    "# 데이터셋 준비\n",
    "all_data = get_custom_dicts(img_dir, json_file)\n",
    "\n",
    "# 1. delete_indices에 해당하는 이미지 제거\n",
    "all_data = [d for d in all_data if d[\"image_id\"] not in delete_indices]\n",
    "\n",
    "\n",
    "train_size = 1000\n",
    "val_size = 184\n",
    "test_size = len(all_data) - (train_size + val_size)\n",
    "\n",
    "train_data, val_data, test_data = split_dataset(all_data, train_size, val_size, seed=42)\n",
    "\n",
    "\n",
    "print(f\"Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}\")\n",
    "\n",
    "# DatasetCatalog 등록 (기존 이름이 있으면 제거)\n",
    "for d in [\"train\", \"val\", \"test\"]:\n",
    "    name = f\"cropped_{d}\"\n",
    "    if name in DatasetCatalog.list():\n",
    "        DatasetCatalog.remove(name)\n",
    "\n",
    "# Detectron2에 등록\n",
    "DatasetCatalog.register(\"cropped_train\", lambda: train_data)\n",
    "DatasetCatalog.register(\"cropped_val\", lambda: val_data)\n",
    "DatasetCatalog.register(\"cropped_test\", lambda: test_data)\n",
    "\n",
    "MetadataCatalog.get(\"cropped_train\").set(thing_classes=class_names)\n",
    "MetadataCatalog.get(\"cropped_val\").set(thing_classes=class_names)\n",
    "MetadataCatalog.get(\"cropped_test\").set(thing_classes=class_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3efd40-d69e-412f-a1e9-b9b0b6c6cd08",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 데이터 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ba79aa-d4be-400f-8565-97fbb6d6988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 바운딩박스 크기 데이터 추출\n",
    "bbox_data = []\n",
    "for data in all_data:\n",
    "    for annotation in data[\"annotations\"]:\n",
    "        x, y, w, h = annotation[\"bbox\"]\n",
    "        bbox_data.append({\"image_id\": data[\"image_id\"], \"bbox_width\": w, \"bbox_height\": h})\n",
    "\n",
    "# DataFrame으로 변환\n",
    "df_bboxes = pd.DataFrame(bbox_data)\n",
    "\n",
    "# 데이터 분포 통계 확인\n",
    "print(\"=== Bounding Box Data ===\")\n",
    "print(df_bboxes[[\"bbox_width\", \"bbox_height\"]].describe())\n",
    "\n",
    "# 데이터 분포 시각화\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.hist(df_bboxes[\"bbox_width\"], bins=20, alpha=0.7, label=\"BBox Width\")\n",
    "plt.hist(df_bboxes[\"bbox_height\"], bins=20, alpha=0.7, label=\"BBox Height\")\n",
    "plt.xlabel(\"Pixels\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Bounding Box Size Distribution\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cc0f0c-0c52-4281-8762-b4d6546ad8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 바운딩박스 크기 데이터 추출 및 리사이즈 적용\n",
    "bbox_data_resized = []\n",
    "for data in all_data:\n",
    "    orig_width, orig_height = data[\"width\"], data[\"height\"]\n",
    "    scale_x = 320 / orig_width\n",
    "    scale_y = 320 / orig_height\n",
    "\n",
    "    for annotation in data[\"annotations\"]:\n",
    "        x, y, w, h = annotation[\"bbox\"]\n",
    "        resized_w = w * scale_x\n",
    "        resized_h = h * scale_y\n",
    "        bbox_data_resized.append({\"image_id\": data[\"image_id\"], \"resized_bbox_width\": resized_w, \"resized_bbox_height\": resized_h})\n",
    "\n",
    "# DataFrame으로 변환\n",
    "df_bboxes_resized = pd.DataFrame(bbox_data_resized)\n",
    "\n",
    "# 데이터 분포 통계 확인\n",
    "print(\"=== Resized Bounding Box Data ===\")\n",
    "print(df_bboxes_resized[[\"resized_bbox_width\", \"resized_bbox_height\"]].describe())\n",
    "\n",
    "# 데이터 분포 시각화\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.hist(df_bboxes_resized[\"resized_bbox_width\"], bins=20, alpha=0.7, label=\"Resized BBox Width\")\n",
    "plt.hist(df_bboxes_resized[\"resized_bbox_height\"], bins=20, alpha=0.7, label=\"Resized BBox Height\")\n",
    "plt.xlabel(\"Pixels\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Resized Bounding Box Size Distribution (320x320)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22206449-e354-49d1-9de5-f5ad4783fd5f",
   "metadata": {},
   "source": [
    "## TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686f2e74-f51d-41c7-b4d6-e537fc4e01a2",
   "metadata": {},
   "source": [
    "#### final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c07fb54-b369-4bc7-9316-76c38e5f10a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# 7) 훈련 실행\n",
    "# ---------------------------------------------------------------------------- #\n",
    "set_seed(seed = 42)\n",
    "\n",
    "output_dir = \"./Crop_OutPut_Folder/finaltest_cropped_output\"\n",
    "\n",
    "# Detectron2 모델 로드\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.WEIGHTS = \"./OutPut_Folder/output_nut_new_train_10000/model_final.pth\"  # 사전 학습 가중치 설정\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "# P2단 추가 설정\n",
    "cfg.MODEL.RESNETS.OUT_FEATURES = [\"res2\", \"res3\", \"res4\", \"res5\"]\n",
    "cfg.MODEL.FPN.IN_FEATURES = [\"res2\"]\n",
    "cfg.MODEL.RETINANET.IN_FEATURES = [\"p2\"]\n",
    "cfg.MODEL.ANCHOR_GENERATOR.SIZES = [[x, x * 3, x * 6] for x in [4]]           \n",
    "\n",
    "# 이미지 크기 설정(Min=800, Max=1333)\n",
    "cfg.INPUT.MIN_SIZE_TEST = 300\n",
    "cfg.INPUT.MAX_SIZE_TEST = 400\n",
    "    \n",
    "cfg.MODEL.RETINANET.NUM_CLASSES = 1\n",
    "cfg.DATASETS.TRAIN = (\"cropped_train\",)\n",
    "cfg.DATASETS.TEST = (\"cropped_val\",)\n",
    "\n",
    "\n",
    "# 학습 기간 연장\n",
    "cfg.SOLVER.IMS_PER_BATCH = 16\n",
    "cfg.SOLVER.MAX_ITER = 18000  # 더 많은 iteration\n",
    "cfg.OUTPUT_DIR = output_dir  # 학습 결과 저장 디렉토리\n",
    "cfg.SOLVER.BASE_LR = 0.00025  # 학습률 설정\n",
    "\n",
    "cfg.SOLVER.STEPS = [6000,12000]\n",
    "\n",
    "cfg.MODEL.DEVICE = \"cuda:0\"\n",
    "\n",
    "\n",
    "# 출력 디렉토리 생성\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = CustomTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bd4ea5-2527-42fb-b6cb-5195e69d5934",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f7d6b327-5d06-41a6-b3dc-b0aa0f878afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Crop_OutPut_Folder/finaltest_cropped_output\n"
     ]
    }
   ],
   "source": [
    "print(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "871fadfe-72a2-4e07-8262-03b9e19951bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[01/22 04:09:56 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from ./Crop_OutPut_Folder/finaltest_cropped_output/model_final.pth ...\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/22 04:09:56 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[01/22 04:09:56 d2.evaluation.coco_evaluation]: \u001b[0mTrying to convert 'cropped_test' to COCO format ...\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/22 04:09:56 d2.data.datasets.coco]: \u001b[0mUsing previously cached COCO format annotations at './Performance_Folder/finaltest_cropped_output/cropped_test_coco_format.json'. You need to clear the cache file if your dataset has been modified.\n",
      "\u001b[32m[01/22 04:09:56 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|   object   | 185          |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[01/22 04:09:56 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(300, 300), max_size=400, sample_style='choice')]\n",
      "\u001b[32m[01/22 04:09:56 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[01/22 04:09:56 d2.data.common]: \u001b[0mSerializing 184 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[01/22 04:09:56 d2.data.common]: \u001b[0mSerialized dataset takes 0.04 MiB\n",
      "\u001b[32m[01/22 04:09:56 d2.evaluation.evaluator]: \u001b[0mStart inference on 184 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/fvcore/common/checkpoint.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[01/22 04:09:57 d2.evaluation.evaluator]: \u001b[0mInference done 11/184. Dataloading: 0.0008 s/iter. Inference: 0.0232 s/iter. Eval: 0.0003 s/iter. Total: 0.0243 s/iter. ETA=0:00:04\n",
      "\u001b[32m[01/22 04:10:02 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:05.061069 (0.028274 s / iter per device, on 1 devices)\n",
      "\u001b[32m[01/22 04:10:02 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:02 (0.016631 s / iter per device, on 1 devices)\n",
      "\u001b[32m[01/22 04:10:02 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[01/22 04:10:02 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./Performance_Folder/finaltest_cropped_output/coco_instances_results.json\n",
      "\u001b[32m[01/22 04:10:02 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[01/22 04:10:02 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[01/22 04:10:02 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.01 seconds.\n",
      "\u001b[32m[01/22 04:10:02 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[01/22 04:10:02 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.436\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.904\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.400\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.379\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.485\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.649\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.529\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.541\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.541\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.474\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.608\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.704\n",
      "\u001b[32m[01/22 04:10:02 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 43.640 | 90.356 | 40.003 | 37.939 | 48.524 | 64.911 |\n",
      "OrderedDict([('bbox', {'AP': 43.63980352565224, 'AP50': 90.3563957496028, 'AP75': 40.002819812491595, 'APs': 37.93910390489315, 'APm': 48.524147271423125, 'APl': 64.91080792429376})])\n"
     ]
    }
   ],
   "source": [
    "# 필요한 모듈 Import\n",
    "from detectron2.engine import DefaultPredictor  # 탐지 모델의 기본 Predictor\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset  # COCO 평가와 데이터셋 추론 함수\n",
    "from detectron2.data import build_detection_test_loader  # 테스트 데이터 로더 생성 함수\n",
    "from pycocotools.coco import COCO  # COCO 데이터셋 처리를 위한 모듈\n",
    "from pycocotools.cocoeval import COCOeval  # COCO 평가 수행을 위한 모듈\n",
    "import numpy as np  # 배열 연산 라이브러리\n",
    "\n",
    "# 출력 디렉토리 설정\n",
    "outdir = \"./Performance_Folder/{}\".format(output_dir.split('/')[2])  # output_dir의 하위 폴더 경로 생성\n",
    "\n",
    "# 모델 설정\n",
    "cfg.MODEL.WEIGHTS = '{}/model_final.pth'.format(output_dir)  # 훈련 완료된 모델 가중치 파일 경로 설정\n",
    "predictor = DefaultPredictor(cfg)  # DefaultPredictor로 모델 로드\n",
    "\n",
    "# 테스트셋 평가 준비\n",
    "evaluator = COCOEvaluator(\"cropped_test\", cfg, False, output_dir=outdir)  # COCO 평가 객체 생성\n",
    "# Custom Mapper를 적용한 테스트 데이터 로더 생성\n",
    "test_loader = build_detection_test_loader(cfg, \"cropped_test\")  # 테스트 데이터 로더에 커스텀 매퍼 적용\n",
    "\n",
    "# 탐지 결과 생성 및 평가 수행\n",
    "print(inference_on_dataset(predictor.model, test_loader, evaluator))  # 테스트셋에 대한 평가 결과 출력 (2열 결과)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b84d12-7a70-495c-b83d-39c6185510d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Performance_Folder/cropped_output_transferlearning/cropped_test_coco_format.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.03s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "COCO Evaluator 결과 출력\n",
      "\n",
      "['object']\n",
      "Category: Hiddenlens - AR (IoU=0.5, area=all): 0.951\n",
      "Category: Hiddenlens - AR (IoU=0.6, area=all): 0.827\n",
      "Category: Hiddenlens - AR (IoU=0.7, area=all): 0.681\n",
      "Category: Hiddenlens - AR (IoU=0.8, area=all): 0.422\n",
      "Category: Hiddenlens - AR (IoU=0.9, area=all): 0.059\n",
      "\n",
      "\n",
      "Average Recall (AR) @ IoU=0.5 | area=small | maxDets=100: 0.937\n",
      "\n",
      "Category: Hiddenlens - AR (IoU=0.5, area=small): 0.937\n",
      "Category: Hiddenlens - AR (IoU=0.6, area=small): 0.793\n",
      "Category: Hiddenlens - AR (IoU=0.7, area=small): 0.577\n",
      "Category: Hiddenlens - AR (IoU=0.8, area=small): 0.297\n",
      "Category: Hiddenlens - AR (IoU=0.9, area=small): 0.009\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "\n",
    "# outdir = output_dir\n",
    "# \"./Crop_OutPut_Folder/{}\".format(output_dir.split('/')[2])\n",
    "gt_file = \"./Crop_OutPut_Folder/{}\".format(output_dir.split('/')[2])   \n",
    "# gt_file = \"./Performance_Folder/lens_8000_P2P3P4_640/custom_test_coco_format.json\"# GT 파일 경로\n",
    "dt_file = f\"{outdir}/coco_instances_results.json\"     # Detection 결과 파일 경로\n",
    "print(gt_file)\n",
    "coco_gt = COCO(gt_file)\n",
    "coco_dt = coco_gt.loadRes(dt_file)\n",
    "\n",
    "coco_eval = COCOeval(coco_gt, coco_dt, \"bbox\")\n",
    "# 평가 실행: 모든 이미지에 대해 매칭, IoU 계산 등 내부 평가 수행\n",
    "coco_eval.evaluate()\n",
    "\n",
    "# 평가 누적: 모든 IoU, 카테고리, 영역 범위, maxDet 조건을 기반으로 결과 집계\n",
    "coco_eval.accumulate()\n",
    "\n",
    "# 1. 기본 summarize 호출로 전체 AP, AR 등 기본 평가 지표(표준 COCO metrics) 출력\n",
    "print(\"COCO Evaluator 결과 출력\")\n",
    "# coco_eval.summarize()\n",
    "print(\"\")\n",
    "\n",
    "recall = coco_eval.eval['recall']\n",
    "iou_thrs = 0.5 + np.arange(10)*0.05\n",
    "\n",
    "cat_ids = coco_gt.getCatIds()\n",
    "cats = coco_gt.loadCats(cat_ids)\n",
    "cat_names = [c['name'] for c in cats]\n",
    "print(cat_names)\n",
    "\n",
    "if recall.size > 0:\n",
    "    desired_ious = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    max_det_idx = 2\n",
    "    \n",
    "    # Hiddenlens 클래스에 대한 area=all AR 계산\n",
    "    if \"object\" in cat_names:\n",
    "        hiddenlens_idx = cat_names.index(\"object\")\n",
    "\n",
    "        # IoU=0.5~0.9, area=all\n",
    "        area_all_idx = 0\n",
    "        for iou_target in desired_ious:\n",
    "            iou_idx_candidates = np.where(np.isclose(iou_thrs, iou_target, atol=1e-6))[0]\n",
    "            if len(iou_idx_candidates) == 0:\n",
    "                print(f\"IoU={iou_target} is not in iou_thrs\")\n",
    "                continue\n",
    "            iou_idx = iou_idx_candidates[0]\n",
    "\n",
    "            cat_vals_hiddenlens = recall[iou_idx, hiddenlens_idx, area_all_idx, max_det_idx]\n",
    "            cat_valid_hiddenlens = cat_vals_hiddenlens[cat_vals_hiddenlens >= 0]\n",
    "\n",
    "            if cat_valid_hiddenlens.size == 0:\n",
    "                print(f\"Category: Hiddenlens - AR (IoU={iou_target}, area=all): No valid data\")\n",
    "            else:\n",
    "                cat_ar_hiddenlens = np.mean(cat_valid_hiddenlens)\n",
    "                print(f\"Category: Hiddenlens - AR (IoU={iou_target}, area=all): {cat_ar_hiddenlens:.3f}\")\n",
    "\n",
    "        print(\"\")\n",
    "\n",
    "        # 먼저 IoU=0.5, area=small일 때 전체 카테고리에 대한 AR 출력\n",
    "        area_small_idx = 1\n",
    "        iou_05_index_candidates = np.where(np.isclose(iou_thrs, 0.5, atol=1e-6))[0]\n",
    "        if len(iou_05_index_candidates) == 0:\n",
    "            print(\"IoU=0.5 is not in iou_thrs\")\n",
    "        else:\n",
    "            iou_05_index = iou_05_index_candidates[0]\n",
    "            vals_small = recall[iou_05_index, :, area_small_idx, max_det_idx]\n",
    "            valid_small_vals = vals_small[vals_small >= 0]\n",
    "            if valid_small_vals.size == 0:\n",
    "                print(\"\\nNo valid data for IoU=0.5 | area=small | maxDets=100\")\n",
    "            else:\n",
    "                ar_small_iou_0_5 = np.mean(valid_small_vals)\n",
    "                print(f\"\\nAverage Recall (AR) @ IoU=0.5 | area=small | maxDets=100: {ar_small_iou_0_5:.3f}\")\n",
    "                print('')\n",
    "\n",
    "        # IoU=0.5~0.9, area=small\n",
    "        for iou_target in desired_ious:\n",
    "            iou_idx_candidates = np.where(np.isclose(iou_thrs, iou_target, atol=1e-6))[0]\n",
    "            if len(iou_idx_candidates) == 0:\n",
    "                print(f\"IoU={iou_target} is not in iou_thrs\")\n",
    "                continue\n",
    "            iou_idx = iou_idx_candidates[0]\n",
    "\n",
    "            cat_vals_hiddenlens = recall[iou_idx, hiddenlens_idx, area_small_idx, max_det_idx]\n",
    "            cat_valid_hiddenlens = cat_vals_hiddenlens[cat_vals_hiddenlens >= 0]\n",
    "\n",
    "            if cat_valid_hiddenlens.size == 0:\n",
    "                print(f\"Category: Hiddenlens - AR (IoU={iou_target}, area=small): No valid data\")\n",
    "            else:\n",
    "                cat_ar_hiddenlens = np.mean(cat_valid_hiddenlens)\n",
    "                print(f\"Category: Hiddenlens - AR (IoU={iou_target}, area=small): {cat_ar_hiddenlens:.3f}\")\n",
    "    else:\n",
    "        print(\"No 'Hiddenlens' category found in annotations.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
